---
title: "DATA 606 Spring 2019 - Final Exam"
author: "Santosh Cheruku"
output: html_document
---

```{r, eval=FALSE, echo=FALSE, tinytex.verbose = TRUE}
options(tinytex.verbose = TRUE)
install.packages("ggplot2")
library(ggplot2)
# The following two commands will install a LaTeX installation so that the document can be compiled to PDF. These only need to be run once per R installation.
#install.packages('tinytex')
#tinytex::install_tinytex()
```

```{r, echo=FALSE}
options(digits = 2)
```

# Part I

Please put the answers for Part I next to the question number (2pts each):

1.  **b** </br>
2.  **a** </br>
    In a left skewed distribution, median is always higher than mean </br>
3.  **a** </br>
    Observational studies cannot prove causality, it has to be randomized. </br>
4.  **c** </br>
    An association can be derived if the test statistic is higher. </br>
5.  **b** </br>
    Outliers will be Q1 - 1.5 X IQR and Q3 + 1.5 X IQR, which will be 17.8 and 69 respectively. </br>
6.  **d** </br>

7a. Describe the two distributions (2pts).

Answer: **Figure A represents a highly right skewed model** </br>
        **Figure B represents a normal distribution model**


7b. Explain why the means of these two distributions are similar but the standard deviations are not (2 pts).

**Answer: The means are similar since the 2nd distribution is a sample derived from the first.**


7c. What is the statistical principal that describes this phenomenon (2 pts)?
    Answer: **Central Limit Theorem describes the phenomenon which says that "given a sufficiently large sample size from a population with a finite level of variance, the mean of all samples from the same population will be approximately equal to the mean of the population"**

# Part II

Consider the four datasets, each with two columns (x and y), provided below. Be sure to replace the `NA` with your answer for each part (e.g. assign the mean of `x` for `data1` to the `data1.x.mean` variable). When you Knit your answer document, a table will be generated with all the answers.

```{r}
options(digits=2)
data1 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68))
data2 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74))
data3 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73))
data4 <- data.frame(x=c(8,8,8,8,8,8,8,19,8,8,8),
					y=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89))
```

For each column, calculate (to two decimal places):

#### a. The mean (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.mean <- mean(data1$x)
data1.y.mean <- mean(data1$y)
data2.x.mean <- mean(data2$x)
data2.y.mean <- mean(data2$y)
data3.x.mean <- mean(data3$x)
data3.y.mean <- mean(data3$y)
data4.x.mean <- mean(data4$x)
data4.y.mean <- mean(data4$y)
```

#### b. The median (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.median <- median(data1$x)
data1.y.median <- median(data1$y)
data2.x.median <- median(data2$x)
data2.y.median <- median(data2$y)
data3.x.median <- median(data3$x)
data3.y.median <- median(data3$y)
data4.x.median <- median(data4$x)
data4.y.median <- median(data4$y)
```

#### c. The standard deviation (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.sd <- sd(data1$x)
data1.y.sd <- sd(data1$y)
data2.x.sd <- sd(data2$x)
data2.y.sd <- sd(data2$y)
data3.x.sd <- sd(data3$x)
data3.y.sd <- sd(data3$y)
data4.x.sd <- sd(data4$x)
data4.y.sd <- sd(data4$y)
```

#### For each x and y pair, calculate (also to two decimal places; 1 pt):

#### d. The correlation (1 pt).

```{r include=TRUE}
data1.correlation <- cor(data1$x, data1$y)
data2.correlation <- cor(data2$x, data2$y)
data3.correlation <- cor(data3$x, data3$y)
data4.correlation <- cor(data4$x, data4$y)
```

#### e. Linear regression equation (2 pts).

```{r include=TRUE}
data1.slope <- coef(lm(data = data1, y~x))["x"]
data2.slope <- coef(lm(data = data2, y~x))["x"]
data3.slope <- coef(lm(data = data3, y~x))["x"]
data4.slope <- coef(lm(data = data4, y~x))["x"]

data1.intercept <- coef(lm(data = data1, y~x))[1]
data2.intercept <- coef(lm(data = data2, y~x))[1]
data3.intercept <- coef(lm(data = data4, y~x))[1]
data4.intercept <- coef(lm(data = data1, y~x))[1]
```

#### f. R-Squared (2 pts).

```{r include=TRUE}
data1.rsquared <- summary(lm(data = data1, y~x))$r.squared
data2.rsquared <- summary(lm(data = data2, y~x))$r.squared
data3.rsquared <- summary(lm(data = data3, y~x))$r.squared
data4.rsquared <- summary(lm(data = data4, y~x))$r.squared
```

```{r, echo=FALSE, results='asis'}
##### DO NOT MODIFY THIS R BLOCK! ######
# This R block will create a table to display all the calculations above in one table.
library(knitr)
library(kableExtra)
results <- data.frame(
	data1.x = c(data1.x.mean, data1.x.median, data1.x.sd, data1.correlation, data1.intercept, data1.slope, data1.rsquared),
	data1.y = c(data1.y.mean, data1.y.median, data1.y.sd, NA, NA, NA, NA),
	data2.x = c(data2.x.mean, data2.x.median, data2.x.sd, data2.correlation, data2.intercept, data2.slope, data2.rsquared),
	data2.y = c(data2.y.mean, data2.y.median, data2.y.sd, NA, NA, NA, NA),
	data3.x = c(data3.x.mean, data3.x.median, data3.x.sd, data3.correlation, data3.intercept, data3.slope, data3.rsquared),
	data3.y = c(data3.y.mean, data3.y.median, data3.y.sd, NA, NA, NA, NA),
	data4.x = c(data4.x.mean, data4.x.median, data4.x.sd, data4.correlation, data4.intercept, data4.slope, data4.rsquared),
	data4.y = c(data4.y.mean, data4.y.median, data4.y.sd, NA, NA, NA, NA),
	stringsAsFactors = FALSE
)
row.names(results) <- c('Mean', 'Median', 'SD', 'r', 'Intercept', 'Slope', 'R-Squared')
names(results) <- c('x','y','x','y','x','y','x','y')
options(knitr.kable.NA = '')
kable(results, digits = 2, 
	  align = 'r',
	  row.names = TRUE, 
	  format.args=list(nsmall = 2)) %>%
	column_spec(2:9, width = ".35in") %>%
	add_header_above(c(" " = 1, "Data 1" = 2, "Data 2" = 2, "Data 3" = 2, "Data 4" = 2))
```

</br>
#### g. For each pair, is it appropriate to estimate a linear regression model? Why or why not? Be specific as to why for each pair and include appropriate plots! (4 pts)


    Answer: The linear regression model fits only if they appear to follow few conditions such as having a correlation, near normal residuals, and a linear or atleast near linear pattern. Here in certain cases it doesn't seem to be the case. 
    
  **Data1: Suitable**
    
```{r warning=F}
library(ggplot2)
cor(data1$x, data1$y)^2
a <- lm(formula = y ~ x, data = data1)
plot(a$residuals)
ggplot(data1, aes(x,y)) + geom_smooth()

```

  **Data2: Not suitable**
    
```{r warning=F}
cor(data2$x, data2$y)^2
a <- lm(formula = y ~ x, data = data2)
plot(a$residuals)
ggplot(data2, aes(x, y)) + geom_smooth()
```

  **Data3: Suitable (Excluding single outlier residual)**
    
```{r warning=F}
cor(data3$x, data3$y)^2
a <- lm(formula = y ~ x, data = data3)
plot(a$residuals)
ggplot(data3, aes(x, y)) + geom_smooth()
```

  **Data4: Not suitable**
    
```{r warning=F}
cor(data4$x, data4$y)^2
a <- lm(formula = y ~ x, data = data4)
plot(a$residuals)
ggplot(data4, aes(x, y)) + geom_smooth()
```


#### h. Explain why it is important to include appropriate visualizations when analyzing data. Include any visualization(s) you create. (2 pts)

    Answer: 
        1) Including appropriate visualizations makes lot of difference analyzing the data because the underlying patterns cannot be identified by looking at large amount of raw data lying in numbers, but if they are converted into a graph or a plot they make more sense.  
        2) The audience always need not be technically proficient in numbers and also may not be willing to spend time to understand them. Therefore, it makes sense to project the same data visually since it's universally well understood and can be easily grasped.


  Below two graphs visually represent the raw numbers and make more sense.
```{r}
data2
```
  


```{r}
ggplot(data2, aes(x, y)) + geom_col()

ggplot(data2, aes(x, y)) + geom_smooth()

```

