{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment – High Frequency Words\n",
    "\n",
    "Group 4\n",
    "- Santosh Cheruku\n",
    "- Vinicio Haro\n",
    "- Javern Wilson\n",
    "- Saayed Alam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement \n",
    "\n",
    "1. Choose a corpus of interest.\n",
    "2. How many total unique words are in the corpus? (Please feel free to define unique words in any interesting,\n",
    "defensible way).\n",
    "3. Taking the most common words, how many unique words represent half of the total words in the corpus?\n",
    "4. Identify the 200 highest frequency words in this corpus.\n",
    "5. Create a graph that shows the relative frequency of these 200 words.\n",
    "6. Does the observed relative frequency of these words follow Zipf’s law? Explain.\n",
    "7. In what ways do you think the frequency of the words in this corpus differ from “all words in all corpora.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download required packages\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import string as str\n",
    "from nltk.stem import PorterStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose a Corpus of Interest\n",
    "\n",
    "Project Guttenberg is a site that allows you to read books directly from the HTML or as a standard txt file on the browser. Because of this, it is an ideal to use as a corpus because we do not need to download data locally. We can pull data directly from the URL thus making this work reproducible. \n",
    "\n",
    "For our corpus, we will be using the book BEALBY by H. G. Wells.\n",
    "\n",
    "http://www.gutenberg.org/files/59769/59769-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/59769/59769-0.txt\"\n",
    "\n",
    "response = urllib.urlopen(url)\n",
    "\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning \n",
    "\n",
    "The raw text file contains the parts of the book that we do not want to use in our analysis. We do not want to use the publication page nor the chapters page. Thankfully we can impliment methodology to choose where and when to start reading the text into our notebook. \n",
    "\n",
    "We will also use the word_tokenize function to convert our raw text into a tokenized version.\n",
    "https://kite.com/python/docs/nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_position = raw.find(\"CHAPTER I\")\n",
    "\n",
    "ending_position = raw.find(\"End of the Project Gutenberg EBook of Bealby; A Holiday, by H. G. Wells\")\n",
    "\n",
    "raw = raw[starting_position:ending_position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have converted the text into tokens, we want to look at the the number of words. In our case we have 83,914 tokenized words. Converting to tokens will make text operations much more efficient. It should be a data prep step when preparing raw data for some downstream analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How many total unique words are in the corpus? (Please feel free to define unique words in any interesting, defensible way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we address the problem of unique words, we want to execute the best practice of cleaning the text data. One example why text should be cleaned is different tenses of the same word such as run and running. If we count them as unqie words, they would be counted as 2 but are they truly unique? No! They mean the same thing in different tenses. We accomplish this by removing the endings of words such as ing , ed, es, and so forth. \n",
    "\n",
    "Another text processing technique we want to use is to make every word lower case. Lets say we have the word \"start\" and somewhere else in the corpuse, we have \"Start.\" By definition, both words are unique because they ae different but in reality, they are the same word except one starts with upper case and the other starts with lower case. \n",
    "\n",
    "We are also going to remove numbers from the raw text. This is a common practice. We do not want to count numbers as words. We can easily do this by applying simple regular expression. \n",
    "\n",
    "Some of the other corpus processing techniques will be to remove whitespace and remove stopwords. \n",
    "\n",
    "We want to define unique words as words that are not stem versions of other words nor common stopwords. \n",
    "\n",
    "Sources:\n",
    "https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\n",
    "https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tok = word_tokenize(raw)\n",
    "\n",
    "len(set(raw_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 9,608 lines without doing anything to the raw text file data. After we convert words to tokens, we will only take alpha numeric words and make all words lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in raw_tok if w.isalpha()] #alphanumeric\n",
    "\n",
    "words = [w.lower() for w in words] #lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() #stemming (remove endings)\n",
    "\n",
    "stem = [ps.stem(w) for w in words]\n",
    "\n",
    "stem = set(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the text to some degree, we are able to count 5,450 unique words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Taking the most common words, how many unique words represent half of the total words in the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great opportunity to apply stop word removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stem2=[w for w in stem if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine the most common words that represent half the total words in the corpus in two ways. Lets see how the counts look when we remove stop works and when we dont. Lets look at when we don't remove stop words first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(words)\n",
    "\n",
    "top_words = fd.most_common(200)\n",
    "\n",
    "i = 0\n",
    "\n",
    "half_words = []\n",
    "\n",
    "for w in top_words:\n",
    "    \n",
    "    if i >= len(raw_tok)/2:\n",
    "        \n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        i = i + w[1]\n",
    "        \n",
    "        half_words.append(w)\n",
    "        \n",
    "pd.DataFrame(half_words, columns = ['Words','Count']).head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(half_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is expected. Without removing any stop words, words such as the or and will appear freqently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(stem2)\n",
    "\n",
    "top_words = fd.most_common(200)\n",
    "\n",
    "i = 0\n",
    "\n",
    "half_words = []\n",
    "\n",
    "for w in top_words:\n",
    "    \n",
    "    if i >= len(raw_tok)/2:\n",
    "        \n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        i = i + w[1]\n",
    "        \n",
    "        half_words.append(w)\n",
    "        \n",
    "pd.DataFrame(half_words, columns = ['Words','Count']).head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(half_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much different picture when we consider removing the stems from common words in addition to the stop words. 166 unique words make half the corpus with stopwords and stems vs 200 unique words that make half the corpus without stems and stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Identify the 200 highest frequency words in this corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From part 3, we know that stop words are most likely going to be present in the top 200 words by frequency. We can verify this with the code below and see how it compares to the stop words and stems removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(words)\n",
    "\n",
    "top_words = fd.most_common(200)\n",
    "\n",
    "print(pd.DataFrame(top_words, columns = ['Word', 'Count']).head(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create a graph that shows the relative frequency of these 200 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure(figsize=(18, 10))\n",
    "\n",
    "plot.xticks(rotation=90)\n",
    "\n",
    "plot.rc('xtick', labelsize=7) \n",
    "\n",
    "plot.title(\"Relative Frequency of Top 200 Words\")\n",
    "\n",
    "fd.plot(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are difficult to see so lets see a more drilled down version of perhaps the top 100 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure(figsize=(18, 10))\n",
    "\n",
    "plot.xticks(rotation=90)\n",
    "\n",
    "plot.rc('xtick', labelsize=10) \n",
    "\n",
    "plot.title(\"Relative Frequency of Top 100 Words\")\n",
    "\n",
    "fd.plot(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Does the observed relative frequency of these words follow Zipf’s law? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first understand what Zipf's law is. Zipf's law states that for any corpus in a natural language system, the word frequency has an inverse relationship to the rank in its frequency distribution. In our system, we see this law at play. Our frequency table shows a word such as \"the\" being the highest ranked. We also know from the law, generally if a word occurs n times,then the next word in the rank occurs n/2 times however as we can see in our frequency plots, this law does not generally apply directly but rather directionaly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. In what ways do you think the frequency of the words in this corpus differ from “all words in all corpora.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that the frequency of words in this corpus, while not exactly matching zipf's law are going to have a very similar distribution. We can even make the hypothesis that the log scale will be close to linear not only in this corupus but all words in corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
