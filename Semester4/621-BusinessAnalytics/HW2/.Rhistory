FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
return(df)
}
df <- func_roc_curve(data)
plot(df$TPR ~ df$FPR, type="s")
plot(df$TPR ~ df$FPR)
df
df <- df[0,0]
df
func_roc_curve <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
df <- data.frame(i=0,TPR=0,FPR=0)
df <- df[0,0]
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$class, data$score_newsclass)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
return(df)
}
df <- func_roc_curve(data)
plot(df$TPR ~ df$FPR)
df
func_roc_curve <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
df <- data.frame(i=0,TPR=0,FPR=0)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$class, data$score_newsclass)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
return(df)
}
df
df <- func_roc_curve(data)
df
library(caret)
data
func_accuracy(data)
func_specificity(data)
func_sensitivty(data)
func_precision(data)
func_class_errorRate(data)
data <- read.csv('classification-output-data.csv', header=T)
confusionMatrix <- table(data$class, data$scored.class)
func_accuracy <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
accuracy <- (TP + TN) / (TP + TN + FP + FN)
return(accuracy)
}
func_class_errorRate <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
classificationErrorRate <- (FP+FN)/sum(confusionMatrix)
return(classificationErrorRate)
}
func_precision <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
TP <- confusionMatrix[1]
FP <- confusionMatrix[3]
precision <- TP / (TP + FP)
return(precision)
}
func_sensitivty <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
TP <- confusionMatrix[1]
FN <- confusionMatrix[2]
sensitivity <- (TP) / (TP + FN)
return(sensitivity)
}
func_specificity <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
specificity <- (TN) / (TN + FP)
return(specificity)
}
func_roc_curve <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
df <- data.frame(i=0,TPR=0,FPR=0)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$class, data$score_newsclass)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
return(df)
}
func_accuracy(data)
func_specificity(data)
func_sensitivty(data)
func_precision(data)
func_class_errorRate(data)
library(caret)
confusionMatrix(confusionMatrix)
library(pROC)
install.packages('pROC')
install.packages("pROC")
roc(data$scored.class, data$class)
plot.roc(data$scored.class, data$class)
library(zoo)
install.packages("zoo")
roc(data$scored.class, data$class)
library(pROC)
roc(data$scored.class, data$class)
plot.roc(data$scored.class, data$class)
plot(roc(dm_df$class, dm_df$scored.probability), main="ROC Curve from pROC Package")
plot(roc(df$class, df$scored.probability), main="ROC Curve from pROC Package")
plot(roc(df$class, df$scored.probability), main="ROC Curve from pROC Package"))
plot(roc(data$class, data$scored.probability), main="ROC Curve from pROC Package")
func_accuracy(data)
func_specificity(data)
func_sensitivty(data)
func_precision(data)
func_class_errorRate(data)
knitr::opts_chunk$set(echo = TRUE)
func_precision <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
TP <- confusionMatrix[1]
FP <- confusionMatrix[3]
precision <- TP / (TP + FP)
return(precision)
}
func_precision <- function(data) {
confusionMatrix <- table(data$class, data$scored.class)
TP <- confusionMatrix[1]
FP <- confusionMatrix[3]
precision <- TP / (TP + FP)
return(precision)
}
func_precision(data)
table(data$class, data$scored.class)
table(data$scored.class, data$class)
confusionMatrix(confusionMatrix)
library(caret)
confusionMatrix(confusionMatrix)
confusionMatrix(data)''
confusionMatrix(data)
data
confusionMatrix(data)
confusionMatrix(data$scored.class, data$class)
confusionMatrix(data=data)
confusionMatrix(data=data, reference=as.factor(data$class))
data$class <- as.factor(data$class)
confusionMatrix(data)
confusionMatrix(data=data, reference=as.factor(data$class))
confusionMatrix(data=data, reference=data$class)
data
str(data)
data$class
confusionMatrix <- table(data$scored.class, data$class)
confusionMatrix
confusionMatrix[1]
confusionMatrix[4]
confusionMatrix[3]
confusionMatrix[2]
func_precision <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
TP <- confusionMatrix[1]
FP <- confusionMatrix[3]
precision <- TP / (TP + FP)
return(precision)
}
func_precision(data)
confusionMatrix <- table(data$scored.class, data$class)
TP <- confusionMatrix[1]
FP <- confusionMatrix[3]
precision <- TP / (TP + FP)
precision
confusionMatrix <- table(data$class, data$scored.class)
TP <- confusionMatrix[1]
FP <- confusionMatrix[3]
precision <- TP / (TP + FP)
precision
confusionMatrix <- table(data$scored.class, data$class)
TP <- confusionMatrix[1]
FP <- confusionMatrix[3]
precision <- TP / (TP + FP)
precision
func_f1Score <- function(data) { (2*func_precision(data)*func_sensitivty(data))/(func_precision(data)+func_sensitivty(data))
}
func_f1Score <- function(data) { return (2*func_precision(data)*func_sensitivty(data))/(func_precision(data)+func_sensitivty(data))
}
func_precision(data) + func_sensitivty(data)
2 * func_precision(data) *  func_sensitivty(data)
(2 * func_precision(data) *  func_sensitivty(data))/(func_precision(data) + func_sensitivty(data))
(2 * func_precision(data) *  func_sensitivty(data))/(func_precision(data) + func_sensitivty(data))
df_table <- with(data, table(scored.class, class)[2:1,2:1])
prec <- (df_table[1])/(df_table[1] + df_table[3])
prec
df_table[1]
df_table[3]
df_table
func_f1Score(data)
return (2 * func_precision(data) *  func_sensitivty(data)) / (func_precision(data) + func_sensitivty(data))
(2 * func_precision(data) *  func_sensitivty(data)) / (func_precision(data) + func_sensitivty(data)
)
func_f1Score <- function(data) { val <- 2 * func_precision(data) *  func_sensitivty(data) / (func_precision(data) + func_sensitivty(data))
}
func_f1Score(data)
func_f1Score <- function(data) { val <- 2 * func_precision(data) *  func_sensitivty(data) / (func_precision(data) + func_sensitivty(data))
return(val)
}
func_f1Score(data)
func_roc_curve(data)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
df
func_roc_curve <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
return(df)
}
df <- func_roc_curve(data)
df
set.seed(1234)
df <- func_roc_curve(data)
df
round(.92877m,2)
round(.92877,2)
func_roc_curve <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,round(TPR,2), round(FPR,2)))
}
df <- na.omit(df)
return(df)
}
df <- func_roc_curve(data)
df
plot(df$FPR, df $FPR)
func_roc_curve <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,round(TPR,4), round(FPR,4)))
}
df <- na.omit(df)
return(df)
}
df <- func_roc_curve(data)
plot(df$FPR, df$FPR)
func_roc_curve <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
return(df)
}
df <- func_roc_curve(data)
plot(df$FPR, df$FPR)
df
data$score_newsclass
func_roc_curve_2 <- function(x) {
seq_int <- seq(0,1,by=0.01)
TPR_vector <- c()
FPR_vector <- c()
for (i in 1:length(seq_int)){
scored_class <- ifelse(x$scored.probability >= seq_int[i], 1, 0)
rev_df <- data.frame(scored.class = scored_class, class = x$class)
df_table <- with(rev_df, table(scored.class, class))
TPR <- (df_table[4])/(df_table[4] + df_table[3])
FPR <- (df_table[2]/(df_table[2] + df_table[1]))
TPR_vector[i] <- TPR
FPR_vector[i] <- FPR
}
return(data.frame(TRUE_POSITIVE = TPR_vector, FALSE_POSITIVE = FPR_vector))
}
df <- func_roc_curve_2(data)
df
plot_df <- df
ROC_plot <- ggplot(plot_df, aes(x=FALSE_POSITIVE, y=TRUE_POSITIVE)) + geom_point() + geom_line(col="blue") + geom_abline(intercept = 0, slope = 1) + labs(title="Receiver Operator Curve", x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)")
AUC_df <- plot_df[complete.cases(plot_df),]
# Now to calculate the AUC
x <- abs(diff(AUC_df$FALSE_POSITIVE))
y <- AUC_df$TRUE_POSITIVE
area_under_curve <- sum(x*y)
func_roc_curve_2 <- function(x) {
seq_int <- seq(0,1,by=0.01)
TPR_vector <- c()
FPR_vector <- c()
for (i in 1:length(seq_int)){
scored_class <- ifelse(x$scored.probability >= seq_int[i], 1, 0)
rev_df <- data.frame(scored.class = scored_class, class = x$class)
df_table <- with(rev_df, table(scored.class, class))
TPR <- (df_table[4])/(df_table[4] + df_table[3])
FPR <- (df_table[2]/(df_table[2] + df_table[1]))
TPR_vector[i] <- TPR
FPR_vector[i] <- FPR
}
plot_df <- data.frame(TRUE_POSITIVE = TPR_vector, FALSE_POSITIVE = FPR_vector)
ROC_plot <- ggplot(plot_df, aes(x=FALSE_POSITIVE, y=TRUE_POSITIVE)) + geom_point() + geom_line(col="blue") + geom_abline(intercept = 0, slope = 1) + labs(title="Receiver Operator Curve", x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)")
# In order to roughly calculate the area under the curve, we must remove the NA values
AUC_df <- plot_df[complete.cases(plot_df),]
# Now to calculate the AUC
x <- abs(diff(AUC_df$FALSE_POSITIVE))
y <- AUC_df$TRUE_POSITIVE
area_under_curve <- sum(x*y)
return(list(ROC_plot, area_under_curve))
}
ROC_list <- roc_curve_fct(dm_df)
ROC_plot <- ROC_list[[1]]
func_roc_curve_2(data)
func_roc_curve <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
ROC_plot <- ggplot(df, aes(x=FPR, y=TPR)) + geom_point() + geom_line(col="blue") + geom_abline(intercept = 0, slope = 1) + labs(title="Receiver Operator Curve", x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)")
# In order to roughly calculate the area under the curve, we must remove the NA values
AUC_df <- df[complete.cases(df),]
# Now to calculate the AUC
x <- abs(diff(AUC_df$FPR))
y <- AUC_df$TPR
area_under_curve <- sum(x*y)
return(list(ROC_plot, area_under_curve))
}
func_roc_curve(data)
AUC_df
func_roc_curve <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
ROC_plot <- ggplot(df, aes(x=FPR, y=TPR)) + geom_point() + geom_line(col="blue") + geom_abline(intercept = 0, slope = 1) + labs(title="Receiver Operator Curve", x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)")
# In order to roughly calculate the area under the curve, we must remove the NA values
AUC_df <- df
# Now to calculate the AUC
x <- abs(diff(AUC_df$FPR))
y <- AUC_df$TPR
area_under_curve <- sum(x*y)
return(list(ROC_plot, area_under_curve))
}
func_roc_curve(data)
diff(AUC_df$FPR)
abs(diff(AUC_df$FPR))
df
df[complete.cases(df),]
df
df <- na.omit(df)
df
AUC_df
abs(diff(df$FPR))
df
df <- na.omit(df)
df
abs(diff(df$FPR))
df
df$FPR
func_roc_curve <- function(data) {
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
ROC_plot <- ggplot(df, aes(x=FPR, y=TPR)) + geom_point() + geom_line(col="blue") + geom_abline(intercept = 0, slope = 1) + labs(title="Receiver Operator Curve", x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)")
# Now to calculate the AUC
x <- abs(diff(df$FPR))
y <- df$TPR
area_under_curve <- sum(x*y)
return(list(ROC_plot, area_under_curve))
}
df <- func_roc_curve(data)
df
plot(roc(data$class, data$scored.probability), main="ROC Curve")
plot(roc(data$scored.probability, data$class), main="ROC Curve")
plot(roc(data$class, data$scored.probability), main="ROC Curve")
plot(roc(data$class, data$scored.probability), main="ROC Curve")
df <- func_roc_curve(data)
df[[1]]
func_roc_curve[[1]]
func_roc_curve(data[[1]]
)
func_roc_curve(data)[[1]]
confusionMatrix <- table(data$scored.class, data$class)
df <- data.frame(i=NA,TPR=NA,FPR=NA)
val <- 0.0
for(i in c(1:99)) {
val <- val + 0.01
data$score_newsclass <- as.numeric(data$scored.probability>val)
confusionMatrix <- table(data$score_newsclass, data$class)
TP <- confusionMatrix[1]
TN <- confusionMatrix[4]
FP <- confusionMatrix[3]
FN <- confusionMatrix[2]
FPR <- FP / (FP + TN)
TPR <- TP / (TP + FN)
df <- rbind(df, c(val,TPR, FPR))
}
df <- na.omit(df)
diff(df$FPR)
?diff
diff(1:10)
abs(diff(df$FPR))
